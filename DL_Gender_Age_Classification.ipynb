{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL-Gender-Age-Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkGVZKNdE6_5"
      },
      "source": [
        "# Gender and Age Classification using CNNs\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u9FmB_SFyBA"
      },
      "source": [
        "La classification automatique par âge et par sexe est devenue pertinente pour un nombre croissant d'applications, en particulier depuis l'essor des plateformes sociales et des médias sociaux. \r\n",
        "\r\n",
        "Dans ce projet Python, nous utiliserons le deep learning pour identifier avec précision le sexe et l'âge d'une personne à partir d'une seule image d'un visage. \r\n",
        "\r\n",
        "Le sexe prédit peut être l'un des Hommes et  Féminin, et l'âge prévu peut être l'une des plages suivantes: (0 - 2), (4 - 6), (8 - 12), (15 - 20) , (25 - 32), (38 - 43), (48 - 53), (60 - 100) (8 nœuds dans la couche softmax finale). Il est très difficile de deviner avec précision un âge exact à partir d'une seule image en raison de facteurs tels que le maquillage, l'éclairage, les obstructions et les expressions faciales. Et donc, nous en faisons un problème de classification au lieu d'en faire un problème de régression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XxgSF4KJfyh"
      },
      "source": [
        "Pour aborder le projet python, nous allons:\r\n",
        "1.   Détecter les visages\r\n",
        "2.   Classer en homme / femme\r\n",
        "3.   Classez-vous dans l'une des 8 tranches d'âge\r\n",
        "4.   Mettez les résultats sur l'image et affichez-la\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99r9DEEKKQjZ"
      },
      "source": [
        "**DATASET**\r\n",
        "\r\n",
        "Pour ce projet Python, nous utiliserons l'ensemble de données Adience.\r\n",
        "\r\n",
        "Cet ensemble de données sert de référence pour les photos de visage et comprend diverses conditions d'imagerie du monde réel telles que le bruit, l'éclairage, la pose et l'apparence. \r\n",
        "\r\n",
        "Les images ont été collectées à partir d'albums Flickr et distribuées sous la licence Creative Commons (CC). Il a un total de 26 580 photos de 2 284 sujets dans huit tranches d'âge (comme mentionné ci-dessus) et mesure environ 1 Go. \r\n",
        "\r\n",
        "Les modèles que nous utiliserons ont été formés sur cet ensemble de données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIdA82S__Iy5"
      },
      "source": [
        "# Age Gender classification from a Youtube Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT5A5nvb4nCh",
        "outputId": "1c93fa3d-f05b-4d93-aca6-1ebcce20b808"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')\r\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTOIGvcZf1Te"
      },
      "source": [
        "!pip install pafy\r\n",
        "!pip install youtube_dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPsjYIW2hGYC"
      },
      "source": [
        "pafy: la bibliothèque Pafy est utilisée pour récupérer le contenu et les métadonnées YouTube (tels que le titre, la note, le nombre de vues, la durée, la note, l'auteur, la vignette, les mots clés, etc.). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "211A8IC8gqVT",
        "outputId": "af9d0826-b21c-4472-ae4c-07d8670ccd24"
      },
      "source": [
        "import pafy\r\n",
        "\r\n",
        "url = 'https://www.youtube.com/watch?v=cWc7vYjgnTs'\r\n",
        "vPafy = pafy.new(url)\r\n",
        "print(vPafy.title)\r\n",
        "print(vPafy.rating)\r\n",
        "print(vPafy.viewcount)\r\n",
        "print(vPafy.author)\r\n",
        "print(vPafy.length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Luciano Pavarotti sings \"Nessun dorma\" from Turandot (The Three Tenors in Concert 1994)\n",
            "4.9185781\n",
            "23187579\n",
            "Warner Classics\n",
            "195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS5Ry9Ofhx79"
      },
      "source": [
        "1. Get the video URL from YouTube.\r\n",
        "2. Face detection with Haar cascades\r\n",
        "3. Gender Recognition with CNN\r\n",
        "4. Age Recognition with CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zeFNLOmw2Gw"
      },
      "source": [
        "1 .prototxt — The definition of CNN goes in here. This file defines the layers in the neural network, each layer’s inputs, outputs and functionality.\r\n",
        "\r\n",
        "2 .caffemodel — This contains the information of the trained neural network (trained model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptV2Dy0S1cxc"
      },
      "source": [
        "Étape 1: Importez toutes les bibliothèques requises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f0KlacExF2d"
      },
      "source": [
        "import cv2\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clIuYvjO1lmr"
      },
      "source": [
        "Étape 2: Obtenez l'URL de la vidéo Youtube et créez un objet «play» qui contient la meilleure résolution de la vidéo au format webm / mp4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLrGuUVA1mvT"
      },
      "source": [
        "url = 'https://www.youtube.com/watch?v=c07IsbSNqfI&feature=youtu.be'\r\n",
        "vPafy = pafy.new(url)\r\n",
        "play = vPafy.getbest(preftype=\"mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYJxUlGX1p9s"
      },
      "source": [
        "Étape 3: Souvent, nous devons capturer le flux en direct avec une caméra. OpenCV fournit une interface très simple à cela. Nous pouvons capturer la vidéo de la caméra, la convertir en vidéo en niveaux de gris et l'afficher. Juste une tâche simple pour commencer.\r\n",
        "\r\n",
        "Pour capturer une vidéo, vous devez créer un objet de capture vidéo. Son argument peut être soit l'index du périphérique, soit le nom d'un fichier vidéo. L'index de l'appareil est juste le numéro pour spécifier quelle caméra. Normalement, une caméra sera connectée (comme dans mon cas). Donc je passe simplement 0 (ou -1). Vous pouvez sélectionner la deuxième caméra en passant 1 et ainsi de suite. Après cela, vous pouvez capturer image par image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWqTiBfg1uML"
      },
      "source": [
        "cap = cv2.VideoCapture(0) #if you are using webcam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL0m_VEb1xZL"
      },
      "source": [
        "Mais dans mon cas, je lis une URL de vidéo en ligne, pour cela, je transmettrai l'objet \"play\" à VideoCapture ()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJtoTnMF10WH"
      },
      "source": [
        "cap = cv2.VideoCapture(play.url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2BLkLjU13DN"
      },
      "source": [
        "Étape 4: à l'aide de set (), je définirai la hauteur et la largeur de notre image vidéo. cap.set (propId, value), ici 3 est le propertyId de width et 4 est pour Height"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNrzs44b16dp",
        "outputId": "dc0132e3-cd3f-40bd-e1d2-e6417b04f8ae"
      },
      "source": [
        "cap.set(3, 480) #set width of the frame\r\n",
        "cap.set(4, 640) #set height of the frame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YuragZW19jJ"
      },
      "source": [
        "Étape 5: Créez 3 listes distinctes pour stocker Model_Mean_Values, Age et Sexe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4AU9ncO2AcX"
      },
      "source": [
        "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\r\n",
        "age_list = ['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)']\r\n",
        "gender_list = ['Male', 'Female']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R21z1_0G2DCI"
      },
      "source": [
        "Étape 6: J'ai défini une fonction pour charger le caffemodel et le prototxt du détecteur d'âge et de sexe, ce sont essentiellement des modèles CNN pré-entraînés qui feront la détection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS9bfRbQ6iDY"
      },
      "source": [
        "path_age_pro=r'/gdrive/My Drive/DL/age_deploy.prototxt'\r\n",
        "path_age_coffemodel=r'/gdrive/My Drive/DL/age_net.caffemodel'\r\n",
        "path_gender_pro=r'/gdrive/My Drive/DL/gender_deploy.prototxt'\r\n",
        "path_gender_coffemodel=r'/gdrive/My Drive/DL/gender_net.caffemodel'\r\n",
        "path_cascade=r'/gdrive/My Drive/DL/haarcascade_frontalface_alt.xml'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba-wxGlz2Hns"
      },
      "source": [
        "def load_caffe_models():\r\n",
        "\r\n",
        " age_net = cv2.dnn.readNetFromCaffe(path_age_pro,path_age_coffemodel)\r\n",
        " gender_net = cv2.dnn.readNetFromCaffe(path_gender_pro, path_gender_coffemodel)\r\n",
        "\r\n",
        " return(age_net, gender_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvdspXzu2Loe"
      },
      "source": [
        "Étape 7: Nous allons maintenant effectuer la détection des visages, la détection de l'âge et la détection du genre et pour cela, nous allons créer une fonction video_detector (age_net, gender_net) dans votre fonction principale et passer age_net et gender_net comme paramètres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5al2-Ud2QCG"
      },
      "source": [
        "age_net, gender_net = load_caffe_models()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "7QhIjsfS-2HI",
        "outputId": "cbf7bdcc-74fb-4861-f34a-a2ce66d02a7b"
      },
      "source": [
        "def video_detector(age_net, gender_net):\r\n",
        "  font = cv2.FONT_HERSHEY_SIMPLEX\r\n",
        "\r\n",
        "while True:\r\n",
        "\r\n",
        "  ret, image = cap.read()\r\n",
        "       \r\n",
        "  face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\r\n",
        " \r\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n",
        "  faces = face_cascade.detectMultiScale(gray, 1.1, 5)\r\n",
        "\r\n",
        "if(len(faces)>0):\r\n",
        "   print(\"Found {} faces\".format(str(len(faces))))\r\n",
        "\r\n",
        "for (x, y, w, h )in faces:\r\n",
        "   cv2.rectangle(image, (x, y), (x+w, y+h), (255, 255, 0), 2)\r\n",
        "\r\n",
        "#Get Face \r\n",
        "   face_img = image[y:y+h, h:h+w].copy()\r\n",
        "   blob = cv2.dnn.blobFromImage(face_img, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\r\n",
        "\r\n",
        "#Predict Gender\r\n",
        "   gender_net.setInput(blob)\r\n",
        "   gender_preds = gender_net.forward()\r\n",
        "   gender = gender_list[gender_preds[0].argmax()]\r\n",
        "   print(\"Gender : \" + gender)\r\n",
        "\r\n",
        "#Predict Age\r\n",
        "   age_net.setInput(blob)\r\n",
        "   age_preds = age_net.forward()\r\n",
        "   age = age_list[age_preds[0].argmax()]\r\n",
        "   print(\"Age Range: \" + age)\r\n",
        "\r\n",
        "overlay_text = \"%s %s\" % (gender, age)\r\n",
        "cv2.putText(image, overlay_text, (x, y), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\r\n",
        "\r\n",
        "cv2.imshow('frame', image)  \r\n",
        "#0xFF is a hexadecimal constant which is 11111111 in binary.\r\n",
        "if cv2.waitKey(1) & 0xFF == ord('q'): \r\n",
        "   break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-359c4d1bfd42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/objdetect/src/cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E45YsZUJ6w_l"
      },
      "source": [
        "video_detector(age_net, gender_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doozb13d2VQY"
      },
      "source": [
        "Étape 8: Lisez l'objet cap créé à partir de VideoCapture () à l'étape 3.\r\n",
        "\r\n",
        "cap.read () renvoie un booléen (True / False). Si le cadre est lu correctement, il sera vrai.\r\n",
        "\r\n",
        "Vous pouvez donc vérifier la fin de la vidéo en vérifiant cette valeur de retour.\r\n",
        "Parfois, le capuchon peut ne pas avoir initialisé la capture. Dans ce cas, ce code affiche une erreur.\r\n",
        "Vous pouvez vérifier s'il est initialisé ou non par la méthode cap.isOpened (). Si c'est vrai, OK. Sinon, ouvrez-le en utilisant cap.open ()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itDInhZ92cNY"
      },
      "source": [
        "ret, image = cap.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR-D4fIr2e5O"
      },
      "source": [
        "Étape 9: Convertissez l'image en image grise car le détecteur de visage OpenCV attend des images grises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzYcXTjP2iVH"
      },
      "source": [
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfZnBcxH2k0z"
      },
      "source": [
        "Étape 10: Chargez le modèle pre-built pour la détection faciale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2L_s6Ci26s4"
      },
      "source": [
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTSk99UH2-K_"
      },
      "source": [
        "Étape 11: Maintenant, comment détecter un visage à partir d'une image à l'aide de CascadeClassifier?\r\n",
        "Eh bien, encore une fois, CascadedClassifier d'OpenCV nous a simplifié la tâche grâce à detectMultiScale (), qui détecte exactement ce dont vous avez besoin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAkW_7kF3FWG"
      },
      "source": [
        "detectMultiScale(image, scaleFactor, minNeighbors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q71d51Rn3QOE"
      },
      "source": [
        "Voici les arguments qui devraient passer à detectMultiScale ().\r\n",
        "\r\n",
        "C'est une fonction générale pour détecter des objets, dans ce cas, elle détectera les visages puisque nous avons appelé dans la cascade de visages. S'il trouve une face, il retourne une liste de positions de ladite face sous la forme «Rect (x, y, w, h).», Sinon, renvoie «Aucun».\r\n",
        "\r\n",
        "Image: la première entrée est l'image en niveaux de gris.\r\n",
        "scaleFactor: Cette fonction compense une fausse perception de taille qui se produit lorsqu'un visage semble être plus grand que l'autre simplement parce qu'il est plus proche de la caméra.\r\n",
        "minNeighbours: algorithme de détection qui utilise une fenêtre mobile pour détecter les objets, il le fait en définissant combien d'objets se trouvent à proximité de l'actuel avant de pouvoir déclarer le visage trouvé."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYl3-yef3Whm"
      },
      "source": [
        "faces = face_cascade.detectMultiScale(gray, 1.1, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFYn7ZSH3asr"
      },
      "source": [
        "Étape 12: Parcourez la liste des visages et dessinez des rectangles sur les visages humains dans la vidéo. Ici, nous recherchons essentiellement des visages, cassons les faces, leurs tailles et dessinons des rectangles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IThOc6zf3Zvv"
      },
      "source": [
        "for (x, y, w, h )in faces:\r\n",
        "   cv2.rectangle(image, (x, y), (x+w, y+h), (255, 255, 0), 2)\r\n",
        "\r\n",
        "# Get Face \r\n",
        "   face_img = image[y:y+h, h:h+w].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pAESCkh3gmJ"
      },
      "source": [
        "Étape 13: OpenCV fournit une fonction pour faciliter le prétraitement des images pour la classification du deep learning: blobFromImage (). Il effectue:\r\n",
        "\r\n",
        "Mean subtraction\r\n",
        "\r\n",
        "Scaling\r\n",
        "\r\n",
        "And optionally channel swapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b16BAz6h3qpz"
      },
      "source": [
        "blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLbE8wZg32ow"
      },
      "source": [
        "\r\n",
        "image: Il s'agit de l'image d'entrée que nous voulons prétraiter avant de la transmettre à notre réseau neuronal profond pour la classification.\r\n",
        "facteur d'échelle: après avoir effectué une soustraction moyenne, nous pouvons éventuellement mettre à l'échelle nos images par un certain facteur. Cette valeur par défaut est 1.0 (c'est-à-dire pas de mise à l'échelle) mais nous pouvons également fournir une autre valeur. Il est également important de noter que le facteur d'échelle doit être de 1 / σ car nous multiplions en fait les canaux d'entrée (après soustraction de la moyenne) par le facteur d'échelle.\r\n",
        "size: Ici, nous fournissons la taille spatiale attendue par le réseau de neurones convolutifs. Pour la plupart des réseaux de neurones de pointe actuels, il s'agit de 224 × 224, 227 × 227 ou 299 × 299.\r\n",
        "signifie: ce sont nos valeurs moyennes de soustraction. Ils peuvent être un 3-tuple des moyens RVB ou ils peuvent être une valeur unique, auquel cas la valeur fournie est soustraite de chaque canal de l'image. Si vous effectuez une soustraction moyenne, assurez-vous de fournir les 3 tuples dans l'ordre (R, V, B), en particulier lorsque vous utilisez le comportement par défaut de swapRB = True.\r\n",
        "swapRB: OpenCV suppose que les images sont dans l'ordre des canaux BGR; cependant, la valeur moyenne suppose que nous utilisons l'ordre RVB. Pour résoudre cet écart, nous pouvons permuter les canaux R et B de l'image en définissant cette valeur sur True. Par défaut, OpenCV effectue ce changement de chaîne pour nous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO91TTiA4O9b"
      },
      "source": [
        "blob = cv2.dnn.blobFromImage(face_img, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gX8RkLr4TIr"
      },
      "source": [
        "Etape 14: Predire le sexe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvl7y3BD4WRS"
      },
      "source": [
        "#Predict Gender\r\n",
        "gender_net.setInput(blob)\r\n",
        "gender_preds = gender_net.forward()\r\n",
        "gender = gender_list[gender_preds[0].argmax()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2P7ax4b4ZFh"
      },
      "source": [
        "Etape 15: Predire l'Age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QapAVI5k4cFy"
      },
      "source": [
        "#Predict Age\r\n",
        "age_net.setInput(blob)\r\n",
        "age_preds = age_net.forward()\r\n",
        "age = age_list[age_preds[0].argmax()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujHmJlzN4fa1"
      },
      "source": [
        "Étape 16: Nous devons maintenant mettre du texte sur notre cadre de sortie en utilisant le module putText () d'OpenCV.\r\n",
        "cv2.putText () prend les paramètres comme:\r\n",
        "\r\n",
        "Données textuelles que vous souhaitez écrire\r\n",
        "Positionnez les coordonnées de l'endroit où vous voulez les placer (c'est-à-dire le coin inférieur gauche où les données commencent).\r\n",
        "Type de police (consultez la documentation cv2.putText () pour les polices prises en charge)\r\n",
        "Échelle de police (spécifie la taille de la police)\r\n",
        "des choses régulières comme la couleur, l'épaisseur, le type de ligne, etc. Pour une meilleure apparence, lineType = cv2.LINE_AA est recommandé."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKpoHEj14j1I"
      },
      "source": [
        "overlay_text = \"%s %s\" % (gender, age)\r\n",
        "cv2.putText(image, overlay_text, (x, y), font, 1, (255, 255, 255), 2, cv2.LINE_AA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYSUaTL84n4I"
      },
      "source": [
        "Étape 17: Imprimez enfin votre sortie finale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhkgpQGX4v8b"
      },
      "source": [
        "cv2.imshow('frame', image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ataCenTK5CUO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fxogx9N_U0Q"
      },
      "source": [
        "# Age Gender Classification From The Camera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td6jYa56QNhC"
      },
      "source": [
        "**On repete les étapes en dessus mais cette fois on a la detection à partir d'une Camera**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQTKpk1U_adf"
      },
      "source": [
        "import cv2\r\n",
        "import imutils\r\n",
        "import time\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB0YfUIO_vSF"
      },
      "source": [
        "cap = cv2.VideoCapture(0)\r\n",
        "\r\n",
        "cap.set(3, 480) #set width\r\n",
        "cap.set(4, 640) #set height"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83f6eogv_06h"
      },
      "source": [
        "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\r\n",
        "age_list = ['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)']\r\n",
        "gender_list = ['Male', 'Female']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yiw7GB_q_5du"
      },
      "source": [
        "def initialize_caffe_models():\r\n",
        "\t\r\n",
        "\tage_net = cv2.dnn.readNetFromCaffe(\r\n",
        "\t\t'data/deploy_age.prototxt', \r\n",
        "\t\t'data/age_net.caffemodel')\r\n",
        "\r\n",
        "\tgender_net = cv2.dnn.readNetFromCaffe(\r\n",
        "\t\t'data/deploy_gender.prototxt', \r\n",
        "\t\t'data/gender_net.caffemodel')\r\n",
        "\r\n",
        "\treturn(age_net, gender_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-K_TD5D_8RE"
      },
      "source": [
        "def read_from_camera(age_net, gender_net):\r\n",
        "\tfont = cv2.FONT_HERSHEY_SIMPLEX\r\n",
        "\r\n",
        "\twhile True:\r\n",
        "\r\n",
        "\t\tret, image = cap.read()\r\n",
        "\r\n",
        "\t\tface_cascade = cv2.CascadeClassifier('data/haarcascade_frontalface_alt.xml')\r\n",
        "\r\n",
        "\t\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n",
        "\t\tfaces = face_cascade.detectMultiScale(gray, 1.1, 5)\r\n",
        "\r\n",
        "\t\tif(len(faces)>0):\r\n",
        "\t\t\tprint(\"Found {} faces\".format(str(len(faces))))\r\n",
        "\r\n",
        "\t\tfor (x, y, w, h )in faces:\r\n",
        "\t\t\tcv2.rectangle(image, (x, y), (x+w, y+h), (255, 255, 0), 2)\r\n",
        "\r\n",
        "\t\t\t# Get Face \r\n",
        "\t\t\tface_img = image[y:y+h, h:h+w].copy()\r\n",
        "\t\t\tblob = cv2.dnn.blobFromImage(face_img, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\r\n",
        "\r\n",
        "\t\t\t#Predict Gender\r\n",
        "\t\t\tgender_net.setInput(blob)\r\n",
        "\t\t\tgender_preds = gender_net.forward()\r\n",
        "\t\t\tgender = gender_list[gender_preds[0].argmax()]\r\n",
        "\t\t\tprint(\"Gender : \" + gender)\r\n",
        "\r\n",
        "\t\t\t#Predict Age\r\n",
        "\t\t\tage_net.setInput(blob)\r\n",
        "\t\t\tage_preds = age_net.forward()\r\n",
        "\t\t\tage = age_list[age_preds[0].argmax()]\r\n",
        "\t\t\tprint(\"Age Range: \" + age)\r\n",
        "\r\n",
        "\t\t\toverlay_text = \"%s %s\" % (gender, age)\r\n",
        "\t\t\tcv2.putText(image, overlay_text, (x, y), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\r\n",
        "\r\n",
        "\r\n",
        "\t\tcv2.imshow('frame', image)\r\n",
        "\r\n",
        "\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\r\n",
        "\t\t\tbreak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o0FpYtFAEl2"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "\tage_net, gender_net = initialize_caffe_models()\r\n",
        "\r\n",
        "\tread_from_camera(age_net, gender_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKmAe0CvKpvJ"
      },
      "source": [
        "#  Predict Age Gender from pictures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFWWqm_6Qikl"
      },
      "source": [
        "**On repete les étapes precedent sur une image données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoRZTFWHf6dL"
      },
      "source": [
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQOCZ0ZagBpS"
      },
      "source": [
        "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\r\n",
        "age_list = ['(0, 2)', '(4, 6)', '(8, 12)', '(15, 20)', '(25, 32)', '(38, 43)', '(48, 53)', '(60, 100)']\r\n",
        "gender_list = ['Male', 'Female']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-LhnnGxNhUI"
      },
      "source": [
        "path_age_pro=r'/gdrive/My Drive/DL/age_deploy.prototxt'\r\n",
        "path_age_coffemodel=r'/gdrive/My Drive/DL/age_net.caffemodel'\r\n",
        "path_gender_pro=r'/gdrive/My Drive/DL/gender_deploy.prototxt'\r\n",
        "path_gender_coffemodel=r'/gdrive/My Drive/DL/gender_net.caffemodel'\r\n",
        "path_cascade=r'/gdrive/My Drive/DL/haarcascade_frontalface_alt.xml'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg-sCx8IgGFE"
      },
      "source": [
        "def initialize_caffe_models():\r\n",
        "\t\r\n",
        "\tage_net = cv2.dnn.readNetFromCaffe(\r\n",
        "\t\tpath_age_pro, \r\n",
        "\t\tpath_age_coffemodel)\r\n",
        "\r\n",
        "\tgender_net = cv2.dnn.readNetFromCaffe(\r\n",
        "\t\tpath_gender_pro, \r\n",
        "\t\tpath_gender_coffemodel)\r\n",
        "\r\n",
        "\treturn(age_net, gender_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc5FoLyagP7H"
      },
      "source": [
        "# read from image code\r\n",
        "face=r'/gdrive/My Drive/DL/girl2.jpg'\r\n",
        "def read_from_image(age_net,gender_net):\r\n",
        "  font=cv2.FONT_HERSHEY_SIMPLEX\r\n",
        "  image=cv2.imread(face)\r\n",
        "  face_cascade=cv2.CascadeClassifier(path_cascade)\r\n",
        "  gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\r\n",
        "  faces=face_cascade.detectMultiscale(gray,1.1,5)\r\n",
        "  if(len(faces)>0):\r\n",
        "    print(\"found {} faces\".format(str(len(faces))))\r\n",
        "  for (x,y,w,h) in faces:\r\n",
        "    cv2.rectangle(image,(x,y),(x+w,y+h),(255,255,0),2)\r\n",
        "  #Get Face \r\n",
        "  face_img=image[y:y+h,h:h+w].copy()\r\n",
        "  blob=cv2.dnn.blobFromImage(face_img,1,(227,227),MODEL_MEAN_VALUES,swapRB=False)\r\n",
        "  #Predict Gender\r\n",
        "  gender_net.setInput(blob)\r\n",
        "  gender_preds=gender_net.forward()\r\n",
        "  gender=gender_list[gender_preds[0].argmax]\r\n",
        "  print(\"Gender:\"+ gender)\r\n",
        "  #Predict Age\r\n",
        "  age_net.setInput(blob)\r\n",
        "  age_preds=age_net.forward()\r\n",
        "  age=age_list[age_preds[0].argmax]\r\n",
        "  print(\"Age Range:\"+ age)\r\n",
        "  overlay_text=\"%S %S\"%(gender,age)\r\n",
        "  cv2.putText(image,overlay_text,(x,y),font,0.5,(100,100,255),2,cv2.LINE_AA)\r\n",
        "  cv2.imshow(\"\",image)\r\n",
        "  cv2.waitKey(0)\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J21l4-UagWJI"
      },
      "source": [
        "age_net, gender_net = initialize_caffe_models()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K2qpwUbOgZp"
      },
      "source": [
        "read_from_image(age_net, gender_net)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}