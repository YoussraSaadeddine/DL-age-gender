{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Format of the file: age_gender_ethnicity_datetime "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/utkface-new/UTKFace/\"\nfiles = os.listdir(path)\nsize = len(files)\nprint(\"Total samples:\",size)\nprint(files[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimages = []\nages = []\ngenders = []\nfor file in files:\n    image = cv2.imread(path+file,0)\n    image = cv2.resize(image,dsize=(64,64))\n    image = image.reshape((image.shape[0],image.shape[1],1))\n    images.append(image)\n    split_var = file.split('_')\n    ages.append(split_var[0])\n    genders.append(int(split_var[1]) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nx_ages = list(set(ages))\ny_ages = [ages.count(i) for i in x_ages]\nplt.bar(x_ages,y_ages)\nplt.show()\nprint(\"Max value:\",max(ages))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display(img):\n    plt.imshow(img[:,:,0])\n    plt.set_cmap('gray')\n    plt.show()\nidx = 500\nsample = images[idx]\nprint(\"Gender:\",genders[idx],\"Age:\",ages[idx])\ndisplay(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def age_group(age):\n    if age >=0 and age < 18:\n        return 1\n    elif age < 30:\n        return 2\n    elif age < 80:\n        return 3\n    else:\n        return 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pre processing\ntarget = np.zeros((size,2),dtype='float32')\nfeatures = np.zeros((size,sample.shape[0],sample.shape[1],1),dtype = 'float32')\nfor i in range(size):\n    target[i,0] = age_group(int(ages[i])) / 4\n    target[i,1] = int(genders[i])\n    features[i] = images[i]\nfeatures = features / 255\ndisplay(features[550])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2,shuffle  = True)\nprint(\"Samples in Training:\",x_train.shape[0])\nprint(\"Samples in Testing:\",x_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of image:\",sample.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras \nfrom keras.layers import *\nfrom keras.models import *\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input(shape=(64,64,1))\nconv1 = Conv2D(32, kernel_size=(3, 3),activation='relu')(inputs)\nconv2 = Conv2D(64, kernel_size=(3, 3),activation='relu')(conv1)\npool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\nconv3 = Conv2D(128, kernel_size=(3, 3),activation='relu')(pool1)\npool2 = MaxPooling2D(pool_size=(2, 2))(conv3)\nconv4 = Conv2D(256, kernel_size=(3,3), activation = 'relu')(pool2)\nconv5 = Conv2D(256, kernel_size=(3,3), activation = 'relu')(conv4)\nflat = Flatten()(conv5)\n\nage_conv1 = Dense(256, activation='relu')(flat)\nage_conv2 = Dense(128, activation='relu')(age_conv1)\nage_conv3 = Dense(64, activation='relu')(age_conv2)\nage_conv4 = Dense(32, activation='relu')(age_conv3)\nage_model = Dense(1, activation='softmax')(age_conv4)\n\n\ngender_conv1 = Dense(256, activation='relu')(flat)\ngender_conv2 = Dense(128, activation='relu')(gender_conv1)\ngender_conv3 = Dense(64, activation='relu')(gender_conv2)\ngender_conv4 = Dense(32, activation='relu')(gender_conv3)\ngender_conv5 = Dense(16, activation='relu')(gender_conv4)\ngender_conv6 = Dense(8, activation='relu')(gender_conv5)\ngender_model = Dense(1, activation='sigmoid')(gender_conv6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(inputs=inputs, outputs=[age_model,gender_model])\nmodel.compile(optimizer = 'adam', loss =['mse','binary_crossentropy'],metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.keras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h = model.fit(x_train,[y_train[:,0],y_train[:,1]],\n              validation_data=(x_test,[y_test[:,0],y_test[:,1]]),\n              epochs = 25, batch_size=128,shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualisation "},{"metadata":{"trusted":true},"cell_type":"code","source":"history = h\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('loss function')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display(img):\n    plt.imshow(img[:,:,0])\n    plt.set_cmap('gray')\n    plt.show()\n    \ndef age_group(age):\n    if age >=0 and age < 18:\n        return 1\n    elif age < 30:\n        return 2\n    elif age < 80:\n        return 3\n    else:\n        return 4\n\ndef get_age(distr):\n    distr = distr*4\n    if distr >= 0.65 and distr <= 1.4:return \"0-18\"\n    if distr >= 1.65 and distr <= 2.4:return \"19-30\"\n    if distr >= 2.65 and distr <= 3.4:return \"31-80\"\n    if distr >= 3.65 and distr <= 4.4:return \"80 +\"\n    return \"Unknown\"\n    \ndef get_gender(prob):\n    if prob < 0.5:return \"Male\"\n    else: return \"Female\"\n\ndef get_result(sample):\n    sample = sample/255\n    val = model.predict( np.array([sample]) )\n    age = get_age(val[0][0])\n    gender = get_gender(val[1][0])\n    print(\"Values:\",val,\"\\nPredicted Gender:\",gender,\"Predicted Age:\",age)\n    \n    \nindexes = [500,59,80,2,4546,7,9,256,45]\nfor idx in indexes:\n    sample = images[idx]\n    display(sample)\n    print(\"Actual Gender:\",get_gender(genders[idx]),\"Age:\",ages[idx])\n    res = get_result(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# ****Gender Classification with Transfer Learning****"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tensorflow.keras.preprocessing.image import img_to_array,load_img\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_list = os.listdir(path) #Go path and list files\nprint(\"Number Of Ä°mages: \", len(file_list))\n#Gender\ngender = [i.split('_')[1] for i in file_list]\ngender_labels= [\"Male\",\"Female\"]\n\ngender_classes = []\ny_gender= []\nfor i in gender:\n    i= int(i)\n    if i== 0:\n        gender_classes.append(0)\n    else:\n        gender_classes.append(1)\n    y_gender.append(i)\n    \ngender_classes= np.array(gender_classes)\nprint(\"gender_classes shape: \",gender_classes.shape) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert Images to Vector and Resize"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data=[]\nfor file in file_list:\n    img= cv2.imread(path+'/'+file)\n    img=cv2.resize(img,(48,48)) # (200,200)--->(48,48)\n    x_data.append(img)\nx_data= np.array(x_data)\nprint(\"x_data shape: \",x_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x_data,gender_classes,test_size=0.2,\n                                               shuffle=True,random_state=42)\nprint(\"Samples in Training:\",x_train.shape)\nprint(\"Samples in Testing:\",x_test.shape)\n\ny_train= to_categorical(y_train,num_classes=2) #one hot encoding\ny_test= to_categorical(y_test,num_classes=2) #[1,0, ...]---> [[1,0],[0,1], ...]\n\nprint(\"y_train shape: \",y_train.shape)\nprint(\"y_test shape: \",y_test.shape)\n\ninput_shape= x_train.shape[1:] #we don't take the number of samples\nprint(\"input shape: \",input_shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## VGG-16 Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% vgg16\nfrom keras.applications.vgg16 import VGG16 \n\nvgg= VGG16(include_top= False,weights=\"imagenet\",input_shape=input_shape)\n\n#include_top=False bencause I'll modify fully connected layers\n# weights='imagenet' because I want the weights used in the imagenet competition\nprint(vgg.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transfer VGG-16 Model Layers to Our Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg_layer_list= vgg.layers #I keep the layers in the vgg_layer_list\n\nmodel= Sequential() #I create my model\n\nfor layer in vgg_layer_list: \n    model.add(layer)        # I transfer vgg layers to my model\n\nfor layer in model.layers:\n    layer.trainable= False  #I'll use vgg16 weights so trainable=False\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation=\"relu\"))\nmodel.add(layers.Dense(128, activation=\"relu\"))\nmodel.add(layers.Dense(64, activation=\"relu\"))\nmodel.add(layers.Dense(32, activation=\"relu\"))\nmodel.add(layers.Dense(2, activation=\"sigmoid\"))\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create Validation Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val= x_train[12000:]\npartial_x_train=x_train[:12000]\n\ny_val= y_train[12000:]\npartial_y_train=y_train[:12000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compile and fit Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\",\n              optimizer= optimizers.RMSprop(lr=1e-5),\n              metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist= model.fit(partial_x_train,partial_y_train,\n                validation_data=(x_val,y_val),\n                epochs=50,\n                batch_size= 512)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train and Validation Loss\nplt.plot(hist.history[\"loss\"],label=\"train loss\")\nplt.plot(hist.history[\"val_loss\"],label=\"val loss\")\nplt.legend()\nfig1 = plt.gcf() # I create figure because Ä± want to save\nplt.show()\nplt.draw()\nfig1.savefig('genderclfloss.png')\n\nplt.figure()\n\n##Train and Validation Accuracy\nplt.plot(hist.history[\"accuracy\"],label=\"train acc\")\nplt.plot(hist.history[\"val_accuracy\"],label=\"val acc\")\nplt.legend()\n\nfig2 = plt.gcf()\nplt.show()\nplt.draw()\nfig2.savefig('genderclfacc.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}